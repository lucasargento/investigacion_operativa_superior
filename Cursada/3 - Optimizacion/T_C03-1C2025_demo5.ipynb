{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMO 5: ENTRENAMIENTO DEL MODELO DE IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import torch   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CARGA Y PREPROCESAMIENTO DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('T_C03-1C2025_historico_prod.csv', sep=';')         # carga del archivo CSV\n",
    "\n",
    "df = pd.get_dummies(df, dtype='int')                                # one hot encoding\n",
    "\n",
    "df = df.drop(columns=['año'])                                       # elimino la columna año\n",
    "\n",
    "y = df['ventas_und']                                                # separo y\n",
    "\n",
    "X = df.drop(columns=['ventas_und'])                                 # separo X  \"X.columns.values.tolist()\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENTRENAMIENTO DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 \t Loss: 549838208.0\n",
      "epoch 200 \t Loss: 291998464.0\n",
      "epoch 300 \t Loss: 105895560.0\n",
      "epoch 400 \t Loss: 70174440.0\n",
      "epoch 500 \t Loss: 51397540.0\n",
      "epoch 600 \t Loss: 45489040.0\n",
      "epoch 700 \t Loss: 43806060.0\n",
      "epoch 800 \t Loss: 42710388.0\n",
      "epoch 900 \t Loss: 41686832.0\n",
      "epoch 1000 \t Loss: 40734312.0\n",
      "epoch 1100 \t Loss: 39845364.0\n",
      "epoch 1200 \t Loss: 38992072.0\n",
      "epoch 1300 \t Loss: 38141608.0\n",
      "epoch 1400 \t Loss: 37284920.0\n",
      "epoch 1500 \t Loss: 36413320.0\n",
      "epoch 1600 \t Loss: 35514092.0\n",
      "epoch 1700 \t Loss: 34593020.0\n",
      "epoch 1800 \t Loss: 33657584.0\n",
      "epoch 1900 \t Loss: 32679002.0\n",
      "epoch 2000 \t Loss: 31642654.0\n",
      "epoch 2100 \t Loss: 30604196.0\n",
      "epoch 2200 \t Loss: 29530612.0\n",
      "epoch 2300 \t Loss: 28461244.0\n",
      "epoch 2400 \t Loss: 27337742.0\n",
      "epoch 2500 \t Loss: 26255064.0\n",
      "epoch 2600 \t Loss: 25186464.0\n",
      "epoch 2700 \t Loss: 24175954.0\n",
      "epoch 2800 \t Loss: 22961268.0\n",
      "epoch 2900 \t Loss: 21062174.0\n",
      "epoch 3000 \t Loss: 19384950.0\n",
      "epoch 3100 \t Loss: 17643590.0\n",
      "epoch 3200 \t Loss: 15680605.0\n",
      "epoch 3300 \t Loss: 13905858.0\n",
      "epoch 3400 \t Loss: 12114114.0\n",
      "epoch 3500 \t Loss: 10330822.0\n",
      "epoch 3600 \t Loss: 8649243.0\n",
      "epoch 3700 \t Loss: 7076593.0\n",
      "epoch 3800 \t Loss: 5739587.5\n",
      "epoch 3900 \t Loss: 4732212.0\n",
      "epoch 4000 \t Loss: 4056958.5\n",
      "epoch 4100 \t Loss: 3605193.25\n",
      "epoch 4200 \t Loss: 3312749.25\n",
      "epoch 4300 \t Loss: 3110243.25\n",
      "epoch 4400 \t Loss: 2976914.25\n",
      "epoch 4500 \t Loss: 2885125.25\n",
      "epoch 4600 \t Loss: 2812752.0\n",
      "epoch 4700 \t Loss: 2733442.5\n",
      "epoch 4800 \t Loss: 2671411.5\n",
      "epoch 4900 \t Loss: 2625234.0\n",
      "epoch 5000 \t Loss: 2587294.25\n",
      "epoch 5100 \t Loss: 2555880.75\n",
      "epoch 5200 \t Loss: 2528373.75\n",
      "epoch 5300 \t Loss: 2506405.0\n",
      "epoch 5400 \t Loss: 2484963.5\n",
      "epoch 5500 \t Loss: 2464705.5\n",
      "epoch 5600 \t Loss: 2446904.5\n",
      "epoch 5700 \t Loss: 2431227.5\n",
      "epoch 5800 \t Loss: 2416494.0\n",
      "epoch 5900 \t Loss: 2403668.25\n",
      "epoch 6000 \t Loss: 2390977.0\n",
      "epoch 6100 \t Loss: 2378450.0\n",
      "epoch 6200 \t Loss: 2367406.0\n",
      "epoch 6300 \t Loss: 2355184.0\n",
      "epoch 6400 \t Loss: 2340787.25\n",
      "epoch 6500 \t Loss: 2329517.75\n",
      "epoch 6600 \t Loss: 2320166.0\n",
      "epoch 6700 \t Loss: 2311676.25\n",
      "epoch 6800 \t Loss: 2305127.25\n",
      "epoch 6900 \t Loss: 2298428.5\n",
      "epoch 7000 \t Loss: 2292594.0\n",
      "epoch 7100 \t Loss: 2287117.5\n",
      "epoch 7200 \t Loss: 2281849.0\n",
      "epoch 7300 \t Loss: 2276412.0\n",
      "epoch 7400 \t Loss: 2271519.75\n",
      "epoch 7500 \t Loss: 2266930.75\n",
      "epoch 7600 \t Loss: 2261692.25\n",
      "epoch 7700 \t Loss: 2256697.0\n",
      "epoch 7800 \t Loss: 2252848.25\n",
      "epoch 7900 \t Loss: 2248851.75\n",
      "epoch 8000 \t Loss: 2244668.0\n",
      "epoch 8100 \t Loss: 2240793.75\n",
      "epoch 8200 \t Loss: 2236561.5\n",
      "epoch 8300 \t Loss: 2232186.5\n",
      "epoch 8400 \t Loss: 2227982.75\n",
      "epoch 8500 \t Loss: 2223762.25\n",
      "epoch 8600 \t Loss: 2220432.0\n",
      "epoch 8700 \t Loss: 2217314.5\n",
      "epoch 8800 \t Loss: 2213404.75\n",
      "epoch 8900 \t Loss: 2210339.0\n",
      "epoch 9000 \t Loss: 2207376.25\n",
      "epoch 9100 \t Loss: 2204430.5\n",
      "epoch 9200 \t Loss: 2201293.5\n",
      "epoch 9300 \t Loss: 2199207.75\n",
      "epoch 9400 \t Loss: 2195724.5\n",
      "epoch 9500 \t Loss: 2192770.0\n",
      "epoch 9600 \t Loss: 2189558.75\n",
      "epoch 9700 \t Loss: 2185890.0\n",
      "epoch 9800 \t Loss: 2181552.0\n",
      "epoch 9900 \t Loss: 2178322.5\n",
      "epoch 10000 \t Loss: 2174627.75\n",
      "epoch 10100 \t Loss: 2171845.25\n",
      "epoch 10200 \t Loss: 2168616.75\n",
      "epoch 10300 \t Loss: 2164283.5\n",
      "epoch 10400 \t Loss: 2159174.25\n",
      "epoch 10500 \t Loss: 2154191.75\n",
      "epoch 10600 \t Loss: 2150770.5\n",
      "epoch 10700 \t Loss: 2146321.25\n",
      "epoch 10800 \t Loss: 2143047.5\n",
      "epoch 10900 \t Loss: 2139822.25\n",
      "epoch 11000 \t Loss: 2137381.25\n",
      "epoch 11100 \t Loss: 2134606.5\n",
      "epoch 11200 \t Loss: 2131125.75\n",
      "epoch 11300 \t Loss: 2128265.5\n",
      "epoch 11400 \t Loss: 2126078.75\n",
      "epoch 11500 \t Loss: 2123086.25\n",
      "epoch 11600 \t Loss: 2120698.0\n",
      "epoch 11700 \t Loss: 2118306.5\n",
      "epoch 11800 \t Loss: 2115813.25\n",
      "epoch 11900 \t Loss: 2113141.0\n",
      "epoch 12000 \t Loss: 2111005.5\n",
      "epoch 12100 \t Loss: 2109774.75\n",
      "epoch 12200 \t Loss: 2106631.25\n",
      "epoch 12300 \t Loss: 2104985.0\n",
      "epoch 12400 \t Loss: 2102229.25\n",
      "epoch 12500 \t Loss: 2099622.0\n",
      "epoch 12600 \t Loss: 2097286.25\n",
      "epoch 12700 \t Loss: 2095276.125\n",
      "epoch 12800 \t Loss: 2093321.75\n",
      "epoch 12900 \t Loss: 2091185.25\n",
      "epoch 13000 \t Loss: 2088245.125\n",
      "epoch 13100 \t Loss: 2085752.25\n",
      "epoch 13200 \t Loss: 2082924.25\n",
      "epoch 13300 \t Loss: 2081104.0\n",
      "epoch 13400 \t Loss: 2078043.125\n",
      "epoch 13500 \t Loss: 2074640.875\n",
      "epoch 13600 \t Loss: 2072585.25\n",
      "epoch 13700 \t Loss: 2070456.375\n",
      "epoch 13800 \t Loss: 2068948.875\n",
      "epoch 13900 \t Loss: 2068114.5\n",
      "epoch 14000 \t Loss: 2065214.875\n",
      "epoch 14100 \t Loss: 2063593.125\n",
      "epoch 14200 \t Loss: 2060885.875\n",
      "epoch 14300 \t Loss: 2058893.125\n",
      "epoch 14400 \t Loss: 2057232.625\n",
      "epoch 14500 \t Loss: 2053745.625\n",
      "epoch 14600 \t Loss: 2051173.625\n",
      "epoch 14700 \t Loss: 2049003.5\n",
      "epoch 14800 \t Loss: 2046710.25\n",
      "epoch 14900 \t Loss: 2045039.125\n",
      "epoch 15000 \t Loss: 2042603.5\n",
      "epoch 15100 \t Loss: 2040161.5\n",
      "epoch 15200 \t Loss: 2038314.125\n",
      "epoch 15300 \t Loss: 2036181.625\n",
      "epoch 15400 \t Loss: 2034732.0\n",
      "epoch 15500 \t Loss: 2032753.875\n",
      "epoch 15600 \t Loss: 2031117.875\n",
      "epoch 15700 \t Loss: 2029871.5\n",
      "epoch 15800 \t Loss: 2027767.875\n",
      "epoch 15900 \t Loss: 2025812.625\n",
      "epoch 16000 \t Loss: 2024765.75\n",
      "epoch 16100 \t Loss: 2022388.625\n",
      "epoch 16200 \t Loss: 2021437.875\n",
      "epoch 16300 \t Loss: 2018721.375\n",
      "epoch 16400 \t Loss: 2016848.0\n",
      "epoch 16500 \t Loss: 2014674.5\n",
      "epoch 16600 \t Loss: 2013427.125\n",
      "epoch 16700 \t Loss: 2011309.75\n",
      "epoch 16800 \t Loss: 2010194.625\n",
      "epoch 16900 \t Loss: 2008840.875\n",
      "epoch 17000 \t Loss: 2008105.5\n",
      "epoch 17100 \t Loss: 2005264.625\n",
      "epoch 17200 \t Loss: 2004598.5\n",
      "epoch 17300 \t Loss: 2002438.875\n",
      "epoch 17400 \t Loss: 2001128.375\n",
      "epoch 17500 \t Loss: 2000699.875\n",
      "epoch 17600 \t Loss: 1999276.375\n",
      "epoch 17700 \t Loss: 1997270.75\n",
      "epoch 17800 \t Loss: 1996399.75\n",
      "epoch 17900 \t Loss: 1994777.5\n",
      "epoch 18000 \t Loss: 1993919.25\n",
      "epoch 18100 \t Loss: 1993130.75\n",
      "epoch 18200 \t Loss: 1990865.125\n",
      "epoch 18300 \t Loss: 1989466.0\n",
      "epoch 18400 \t Loss: 1988467.75\n",
      "epoch 18500 \t Loss: 1987674.5\n",
      "epoch 18600 \t Loss: 1985270.75\n",
      "epoch 18700 \t Loss: 1983957.5\n",
      "epoch 18800 \t Loss: 1982438.875\n",
      "epoch 18900 \t Loss: 1981112.625\n",
      "epoch 19000 \t Loss: 1978271.625\n",
      "epoch 19100 \t Loss: 1976623.0\n",
      "epoch 19200 \t Loss: 1976094.75\n",
      "epoch 19300 \t Loss: 1973140.875\n",
      "epoch 19400 \t Loss: 1971534.5\n",
      "epoch 19500 \t Loss: 1969855.5\n",
      "epoch 19600 \t Loss: 1968526.0\n",
      "epoch 19700 \t Loss: 1967294.375\n",
      "epoch 19800 \t Loss: 1966137.75\n",
      "epoch 19900 \t Loss: 1963614.375\n",
      "epoch 20000 \t Loss: 1962120.75\n",
      "epoch 20100 \t Loss: 1960918.25\n",
      "epoch 20200 \t Loss: 1957411.75\n",
      "epoch 20300 \t Loss: 1955743.875\n",
      "epoch 20400 \t Loss: 1954318.875\n",
      "epoch 20500 \t Loss: 1951237.25\n",
      "epoch 20600 \t Loss: 1949479.0\n",
      "epoch 20700 \t Loss: 1947581.875\n",
      "epoch 20800 \t Loss: 1945990.875\n",
      "epoch 20900 \t Loss: 1944041.375\n",
      "epoch 21000 \t Loss: 1943484.0\n",
      "epoch 21100 \t Loss: 1941277.625\n",
      "epoch 21200 \t Loss: 1939702.5\n",
      "epoch 21300 \t Loss: 1937619.5\n",
      "epoch 21400 \t Loss: 1936526.5\n",
      "epoch 21500 \t Loss: 1933551.0\n",
      "epoch 21600 \t Loss: 1932015.625\n",
      "epoch 21700 \t Loss: 1929540.75\n",
      "epoch 21800 \t Loss: 1927086.375\n",
      "epoch 21900 \t Loss: 1924751.0\n",
      "epoch 22000 \t Loss: 1923027.75\n",
      "epoch 22100 \t Loss: 1921576.375\n",
      "epoch 22200 \t Loss: 1919078.25\n",
      "epoch 22300 \t Loss: 1917501.5\n",
      "epoch 22400 \t Loss: 1916108.25\n",
      "epoch 22500 \t Loss: 1914519.375\n",
      "epoch 22600 \t Loss: 1913050.125\n",
      "epoch 22700 \t Loss: 1912043.25\n",
      "epoch 22800 \t Loss: 1909723.625\n",
      "epoch 22900 \t Loss: 1908832.25\n",
      "epoch 23000 \t Loss: 1906838.125\n",
      "epoch 23100 \t Loss: 1906093.625\n",
      "epoch 23200 \t Loss: 1905616.5\n",
      "epoch 23300 \t Loss: 1903008.125\n",
      "epoch 23400 \t Loss: 1901301.375\n",
      "epoch 23500 \t Loss: 1899489.375\n",
      "epoch 23600 \t Loss: 1898050.75\n",
      "epoch 23700 \t Loss: 1895840.375\n",
      "epoch 23800 \t Loss: 1894064.5\n",
      "epoch 23900 \t Loss: 1892127.5\n",
      "epoch 24000 \t Loss: 1890749.875\n",
      "epoch 24100 \t Loss: 1888623.125\n",
      "epoch 24200 \t Loss: 1887072.25\n",
      "epoch 24300 \t Loss: 1886799.5\n",
      "epoch 24400 \t Loss: 1884891.125\n",
      "epoch 24500 \t Loss: 1882719.5\n",
      "epoch 24600 \t Loss: 1881849.875\n",
      "epoch 24700 \t Loss: 1880188.625\n",
      "epoch 24800 \t Loss: 1879095.875\n",
      "epoch 24900 \t Loss: 1878394.375\n",
      "epoch 25000 \t Loss: 1876924.375\n",
      "epoch 25100 \t Loss: 1875505.75\n",
      "epoch 25200 \t Loss: 1874091.375\n",
      "epoch 25300 \t Loss: 1872270.0\n",
      "epoch 25400 \t Loss: 1870928.75\n",
      "epoch 25500 \t Loss: 1869581.0\n",
      "epoch 25600 \t Loss: 1868631.25\n",
      "epoch 25700 \t Loss: 1867133.375\n",
      "epoch 25800 \t Loss: 1867384.375\n",
      "epoch 25900 \t Loss: 1863991.125\n",
      "epoch 26000 \t Loss: 1862657.25\n",
      "epoch 26100 \t Loss: 1862189.75\n",
      "epoch 26200 \t Loss: 1860408.625\n",
      "epoch 26300 \t Loss: 1860640.0\n",
      "epoch 26400 \t Loss: 1857787.0\n",
      "epoch 26500 \t Loss: 1856800.25\n",
      "epoch 26600 \t Loss: 1856160.125\n",
      "epoch 26700 \t Loss: 1854565.125\n",
      "epoch 26800 \t Loss: 1854499.375\n",
      "epoch 26900 \t Loss: 1853579.0\n",
      "epoch 27000 \t Loss: 1852087.125\n",
      "epoch 27100 \t Loss: 1850645.75\n",
      "epoch 27200 \t Loss: 1848746.125\n",
      "epoch 27300 \t Loss: 1847798.75\n",
      "epoch 27400 \t Loss: 1846963.0\n",
      "epoch 27500 \t Loss: 1847427.625\n",
      "epoch 27600 \t Loss: 1845164.125\n",
      "epoch 27700 \t Loss: 1844905.875\n",
      "epoch 27800 \t Loss: 1847111.0\n",
      "epoch 27900 \t Loss: 1842659.625\n",
      "epoch 28000 \t Loss: 1840805.5\n",
      "epoch 28100 \t Loss: 1840477.75\n",
      "epoch 28200 \t Loss: 1838974.75\n",
      "epoch 28300 \t Loss: 1838145.875\n",
      "epoch 28400 \t Loss: 1837740.625\n",
      "epoch 28500 \t Loss: 1836484.125\n",
      "epoch 28600 \t Loss: 1835405.375\n",
      "epoch 28700 \t Loss: 1833838.25\n",
      "epoch 28800 \t Loss: 1832244.375\n",
      "epoch 28900 \t Loss: 1831071.0\n",
      "epoch 29000 \t Loss: 1829954.5\n",
      "epoch 29100 \t Loss: 1829464.875\n",
      "epoch 29200 \t Loss: 1827129.75\n",
      "epoch 29300 \t Loss: 1828161.125\n",
      "epoch 29400 \t Loss: 1824720.625\n",
      "epoch 29500 \t Loss: 1823607.75\n",
      "epoch 29600 \t Loss: 1823074.625\n",
      "epoch 29700 \t Loss: 1822612.5\n",
      "epoch 29800 \t Loss: 1820382.125\n",
      "epoch 29900 \t Loss: 1818445.25\n",
      "epoch 30000 \t Loss: 1817340.75\n",
      "epoch 30100 \t Loss: 1818234.125\n",
      "epoch 30200 \t Loss: 1814903.375\n",
      "epoch 30300 \t Loss: 1813097.375\n",
      "epoch 30400 \t Loss: 1811725.0\n",
      "epoch 30500 \t Loss: 1810088.75\n",
      "epoch 30600 \t Loss: 1809080.375\n",
      "epoch 30700 \t Loss: 1807830.375\n",
      "epoch 30800 \t Loss: 1807902.875\n",
      "epoch 30900 \t Loss: 1806524.75\n",
      "epoch 31000 \t Loss: 1805754.375\n",
      "epoch 31100 \t Loss: 1804231.625\n",
      "epoch 31200 \t Loss: 1805420.625\n",
      "epoch 31300 \t Loss: 1803674.0\n",
      "epoch 31400 \t Loss: 1801716.5\n",
      "epoch 31500 \t Loss: 1801233.625\n",
      "epoch 31600 \t Loss: 1799987.125\n",
      "epoch 31700 \t Loss: 1799953.875\n",
      "epoch 31800 \t Loss: 1795860.5\n",
      "epoch 31900 \t Loss: 1797135.375\n",
      "epoch 32000 \t Loss: 1794892.375\n",
      "epoch 32100 \t Loss: 1792766.25\n",
      "epoch 32200 \t Loss: 1791791.375\n",
      "epoch 32300 \t Loss: 1790380.625\n",
      "epoch 32400 \t Loss: 1789966.75\n",
      "epoch 32500 \t Loss: 1787975.0\n",
      "epoch 32600 \t Loss: 1787675.875\n",
      "epoch 32700 \t Loss: 1785875.125\n",
      "epoch 32800 \t Loss: 1785513.625\n",
      "epoch 32900 \t Loss: 1784910.0\n",
      "epoch 33000 \t Loss: 1784373.875\n",
      "epoch 33100 \t Loss: 1783052.25\n",
      "epoch 33200 \t Loss: 1781854.875\n",
      "epoch 33300 \t Loss: 1780866.875\n",
      "epoch 33400 \t Loss: 1778950.125\n",
      "epoch 33500 \t Loss: 1777092.5\n",
      "epoch 33600 \t Loss: 1776434.0\n",
      "epoch 33700 \t Loss: 1775764.5\n",
      "epoch 33800 \t Loss: 1774401.625\n",
      "epoch 33900 \t Loss: 1775714.125\n",
      "epoch 34000 \t Loss: 1772117.375\n",
      "epoch 34100 \t Loss: 1771121.0\n",
      "epoch 34200 \t Loss: 1770565.75\n",
      "epoch 34300 \t Loss: 1769621.5\n",
      "epoch 34400 \t Loss: 1768081.125\n",
      "epoch 34500 \t Loss: 1767869.625\n",
      "epoch 34600 \t Loss: 1766395.5\n",
      "epoch 34700 \t Loss: 1768091.125\n",
      "epoch 34800 \t Loss: 1764670.375\n",
      "epoch 34900 \t Loss: 1763821.375\n",
      "epoch 35000 \t Loss: 1762730.625\n",
      "epoch 35100 \t Loss: 1762872.5\n",
      "epoch 35200 \t Loss: 1761716.5\n",
      "epoch 35300 \t Loss: 1759661.375\n",
      "epoch 35400 \t Loss: 1760900.125\n",
      "epoch 35500 \t Loss: 1757410.25\n",
      "epoch 35600 \t Loss: 1758279.375\n",
      "epoch 35700 \t Loss: 1756045.625\n",
      "epoch 35800 \t Loss: 1754809.25\n",
      "epoch 35900 \t Loss: 1756738.75\n",
      "epoch 36000 \t Loss: 1752660.875\n",
      "epoch 36100 \t Loss: 1751374.625\n",
      "epoch 36200 \t Loss: 1752371.75\n",
      "epoch 36300 \t Loss: 1749015.25\n",
      "epoch 36400 \t Loss: 1748574.375\n",
      "epoch 36500 \t Loss: 1747172.0\n",
      "epoch 36600 \t Loss: 1747950.625\n",
      "epoch 36700 \t Loss: 1750880.0\n",
      "epoch 36800 \t Loss: 1743844.625\n",
      "epoch 36900 \t Loss: 1744092.125\n",
      "epoch 37000 \t Loss: 1741537.25\n",
      "epoch 37100 \t Loss: 1741163.0\n",
      "epoch 37200 \t Loss: 1740408.625\n",
      "epoch 37300 \t Loss: 1739549.5\n",
      "epoch 37400 \t Loss: 1737593.0\n",
      "epoch 37500 \t Loss: 1736211.125\n",
      "epoch 37600 \t Loss: 1738393.625\n",
      "epoch 37700 \t Loss: 1736105.875\n",
      "epoch 37800 \t Loss: 1732695.25\n",
      "epoch 37900 \t Loss: 1733027.5\n",
      "epoch 38000 \t Loss: 1730294.0\n",
      "epoch 38100 \t Loss: 1728702.25\n",
      "epoch 38200 \t Loss: 1729936.125\n",
      "epoch 38300 \t Loss: 1726774.75\n",
      "epoch 38400 \t Loss: 1725587.25\n",
      "epoch 38500 \t Loss: 1723569.125\n",
      "epoch 38600 \t Loss: 1722579.875\n",
      "epoch 38700 \t Loss: 1720818.5\n",
      "epoch 38800 \t Loss: 1720922.75\n",
      "epoch 38900 \t Loss: 1718645.75\n",
      "epoch 39000 \t Loss: 1718195.25\n",
      "epoch 39100 \t Loss: 1717312.25\n",
      "epoch 39200 \t Loss: 1716871.0\n",
      "epoch 39300 \t Loss: 1711477.0\n",
      "epoch 39400 \t Loss: 1709954.625\n",
      "epoch 39500 \t Loss: 1709523.875\n",
      "epoch 39600 \t Loss: 1708467.875\n",
      "epoch 39700 \t Loss: 1708026.75\n",
      "epoch 39800 \t Loss: 1705060.5\n",
      "epoch 39900 \t Loss: 1703722.625\n",
      "epoch 40000 \t Loss: 1702150.5\n",
      "epoch 40100 \t Loss: 1699766.625\n",
      "epoch 40200 \t Loss: 1699447.75\n",
      "epoch 40300 \t Loss: 1697716.25\n",
      "epoch 40400 \t Loss: 1699996.0\n",
      "epoch 40500 \t Loss: 1695665.75\n",
      "epoch 40600 \t Loss: 1696933.125\n",
      "epoch 40700 \t Loss: 1691841.25\n",
      "epoch 40800 \t Loss: 1691038.625\n",
      "epoch 40900 \t Loss: 1689117.5\n",
      "epoch 41000 \t Loss: 1688126.75\n",
      "epoch 41100 \t Loss: 1687418.25\n",
      "epoch 41200 \t Loss: 1685904.875\n",
      "epoch 41300 \t Loss: 1685106.875\n",
      "epoch 41400 \t Loss: 1684802.125\n",
      "epoch 41500 \t Loss: 1682946.5\n",
      "epoch 41600 \t Loss: 1684887.875\n",
      "epoch 41700 \t Loss: 1680668.25\n",
      "epoch 41800 \t Loss: 1680334.25\n",
      "epoch 41900 \t Loss: 1679768.125\n",
      "epoch 42000 \t Loss: 1677617.125\n",
      "epoch 42100 \t Loss: 1676390.875\n",
      "epoch 42200 \t Loss: 1677000.5\n",
      "epoch 42300 \t Loss: 1674430.375\n",
      "epoch 42400 \t Loss: 1673477.0\n",
      "epoch 42500 \t Loss: 1672893.5\n",
      "epoch 42600 \t Loss: 1674473.125\n",
      "epoch 42700 \t Loss: 1672996.125\n",
      "epoch 42800 \t Loss: 1669517.75\n",
      "epoch 42900 \t Loss: 1668897.25\n",
      "epoch 43000 \t Loss: 1667489.375\n",
      "epoch 43100 \t Loss: 1665502.25\n",
      "epoch 43200 \t Loss: 1666132.125\n",
      "epoch 43300 \t Loss: 1663951.5\n",
      "epoch 43400 \t Loss: 1662769.0\n",
      "epoch 43500 \t Loss: 1662884.5\n",
      "epoch 43600 \t Loss: 1664200.0\n",
      "epoch 43700 \t Loss: 1660423.875\n",
      "epoch 43800 \t Loss: 1658301.25\n",
      "epoch 43900 \t Loss: 1655983.0\n",
      "epoch 44000 \t Loss: 1659543.625\n",
      "epoch 44100 \t Loss: 1657450.125\n",
      "epoch 44200 \t Loss: 1656751.75\n",
      "epoch 44300 \t Loss: 1653817.25\n",
      "epoch 44400 \t Loss: 1653517.125\n",
      "epoch 44500 \t Loss: 1651126.625\n",
      "epoch 44600 \t Loss: 1651776.125\n",
      "epoch 44700 \t Loss: 1649430.125\n",
      "epoch 44800 \t Loss: 1650042.625\n",
      "epoch 44900 \t Loss: 1649570.875\n",
      "epoch 45000 \t Loss: 1649310.125\n",
      "epoch 45100 \t Loss: 1646461.5\n",
      "epoch 45200 \t Loss: 1646750.875\n",
      "epoch 45300 \t Loss: 1644322.125\n",
      "epoch 45400 \t Loss: 1643766.875\n",
      "epoch 45500 \t Loss: 1640706.625\n",
      "epoch 45600 \t Loss: 1641924.0\n",
      "epoch 45700 \t Loss: 1639976.5\n",
      "epoch 45800 \t Loss: 1640378.25\n",
      "epoch 45900 \t Loss: 1636733.875\n",
      "epoch 46000 \t Loss: 1636379.5\n",
      "epoch 46100 \t Loss: 1639784.875\n",
      "epoch 46200 \t Loss: 1636044.25\n",
      "epoch 46300 \t Loss: 1635276.5\n",
      "epoch 46400 \t Loss: 1635778.5\n",
      "epoch 46500 \t Loss: 1631193.75\n",
      "epoch 46600 \t Loss: 1630157.25\n",
      "epoch 46700 \t Loss: 1628277.625\n",
      "epoch 46800 \t Loss: 1629885.25\n",
      "epoch 46900 \t Loss: 1626572.5\n",
      "epoch 47000 \t Loss: 1624041.125\n",
      "epoch 47100 \t Loss: 1623049.375\n",
      "epoch 47200 \t Loss: 1621812.0\n",
      "epoch 47300 \t Loss: 1620501.25\n",
      "epoch 47400 \t Loss: 1622727.375\n",
      "epoch 47500 \t Loss: 1617642.75\n",
      "epoch 47600 \t Loss: 1620302.5\n",
      "epoch 47700 \t Loss: 1618055.0\n",
      "epoch 47800 \t Loss: 1616882.625\n",
      "epoch 47900 \t Loss: 1615430.125\n",
      "epoch 48000 \t Loss: 1617151.625\n",
      "epoch 48100 \t Loss: 1614536.375\n",
      "epoch 48200 \t Loss: 1611232.875\n",
      "epoch 48300 \t Loss: 1610537.75\n",
      "epoch 48400 \t Loss: 1611424.375\n",
      "epoch 48500 \t Loss: 1608749.75\n",
      "epoch 48600 \t Loss: 1609471.5\n",
      "epoch 48700 \t Loss: 1606930.875\n",
      "epoch 48800 \t Loss: 1609101.125\n",
      "epoch 48900 \t Loss: 1605866.75\n",
      "epoch 49000 \t Loss: 1603925.0\n",
      "epoch 49100 \t Loss: 1604929.375\n",
      "epoch 49200 \t Loss: 1605128.0\n",
      "epoch 49300 \t Loss: 1603173.75\n",
      "epoch 49400 \t Loss: 1600306.625\n",
      "epoch 49500 \t Loss: 1599791.25\n",
      "epoch 49600 \t Loss: 1601778.0\n",
      "epoch 49700 \t Loss: 1600089.5\n",
      "epoch 49800 \t Loss: 1598468.625\n",
      "epoch 49900 \t Loss: 1596159.125\n",
      "epoch 50000 \t Loss: 1596093.625\n"
     ]
    }
   ],
   "source": [
    "# escalo los datos\n",
    "\n",
    "e = StandardScaler()\n",
    "X = e.fit_transform(X.values)\n",
    "\n",
    "# divido el dataset en entrenamiento\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=9)\n",
    "\n",
    "# convierto mis arrays en tensores\n",
    "\n",
    "X_train_t = torch.tensor(X_train).float()\n",
    "X_test_t = torch.tensor(X_test).float()\n",
    "y_train_t = torch.tensor(y_train.values).float()\n",
    "y_test_t = torch.tensor(y_test.values).float()\n",
    "y_test_t = y_test_t[:, None]\n",
    "y_train_t = y_train_t[:, None]\n",
    "\n",
    "# definimos e instanciamos el modelo de la red neuronal\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(47, 100),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(100, 50),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(50, 30),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(30, 1),\n",
    ")\n",
    "\n",
    "# defino los hiperparametros de entrenamiento:\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 50000\n",
    "e_print = 100\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "\n",
    "# realizo el entrenamiento (dependiendo el procesador esta operacion tarda unos 6 minutos):\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    y_pred = model(X_train_t)\n",
    "    loss = loss_fn(y_pred, y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % e_print == 0:\n",
    "        print(f\"epoch {epoch} \\t Loss: {round(loss.item(),4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SE GUARDAN LOS PARÁMETROS ENTRENABLES DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'T_C03-1C2025_modelo.pth')\n",
    "\n",
    "# guarda los datos del para reescalar\n",
    "with open('T_C03-1C2025_scaler.pkl','wb') as f:\n",
    "    pickle.dump(e, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
